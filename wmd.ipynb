{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from pyemd import emd\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value(w):\n",
    "    w_string = str(w.encode('utf-8'))\n",
    "    if(w_string in vocab_dict):\n",
    "        return(vocab_dict[w_string])\n",
    "        \n",
    "    return 0\n",
    "\n",
    "def lexicon_rate(lexicon, comment):\n",
    "    vect = CountVectorizer(token_pattern=pattern, strip_accents=None).fit([lexicon, comment])\n",
    "    v_1, v_2 = vect.transform([lexicon, comment])\n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    W_ = W[[check_value(w) for w in vect.get_feature_names()]]\n",
    "    D_ = euclidean_distances(W_)\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max()\n",
    "    lex=emd(v_1, v_2, D_)\n",
    "    return(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2min 12s, sys: 4.08 s, total: 2min 16s\nWall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv = KeyedVectors.load_word2vec_format('embedding/w2v-v1.wordvectors', unicode_errors=\"ignore\")\n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "fp = np.memmap(\"data/embed.dat\", dtype=np.double, mode='w+', shape=wv.vectors_norm.shape)\n",
    "fp[:] = wv.vectors_norm[:]\n",
    "with open(\"data/embed.vocab\", \"w\") as f:\n",
    "    for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "        print(w.encode('utf-8'), file=f)\n",
    "\n",
    "vocab_len = len(wv.vocab)\n",
    "del fp\n",
    "\n",
    "W = np.memmap(\"data/embed.dat\", dtype=np.double, mode=\"r\", shape=(vocab_len, 300))\n",
    "\n",
    "with open(\"data/embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n",
    "vocab_dict={w:k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para cada fonte, percorrer a lista de arquivos em determianda pasta\n",
    "folders = [\n",
    "    'Indicadores/brasil_escola',\n",
    "    # 'Indicadores/clue_words_pt',\n",
    "    'Indicadores/acrobata_das_letras',\n",
    "    'Indicadores/mundo_educacao',\n",
    "    'Indicadores/Subjetivity_lexicon',\n",
    "    'Indicadores/PersuativeEssays_UKP',\n",
    "    'Indicadores/2020_Jonathan'\n",
    "]\n",
    "\n",
    "\n",
    "def get_indicators_by_sources(folders):\n",
    "    indicators = dict()\n",
    "    for folder in folders:\n",
    "        argumentacao = list()\n",
    "        for file_name in os.listdir(folder):    \n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            with open(file_path) as f:\n",
    "                argumentacao += [w.rstrip() for w in f.readlines()]\n",
    "        argumentacao = ' '.join(argumentacao)\n",
    "        indicators[folder.split('/')[1]] = argumentacao\n",
    "    return indicators\n",
    "    \n",
    "def get_all_indicators():\n",
    "    with open('Indicadores/preprocessed_indicators_woclue.txt') as f:\n",
    "        argumentacao = [w.rstrip() for w in f.readlines()]\n",
    "    return ' '.join(argumentacao)\n",
    "\n",
    "# argumentacoes = get_indicators_by_sources(folders)\n",
    "argumentacoes = get_all_indicators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (type(argumentacoes) == dict):\n",
    "    n_folder=6\n",
    "    argumentacao = argumentacoes[folders[n_folder].split('/')[1]]\n",
    "else:\n",
    "    argumentacao = argumentacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # redacoes corretas:\n",
    "# df = pd.read_csv(\"redações/jonathan_ann/redacoes_extraidas_com_nao_argumento.csv\")\n",
    "# content = list(df.T.to_dict().values())\n",
    "\n",
    "## redacoes:\n",
    "# df = pd.read_csv(\"redações/redacoes_extraidas.csv\")\n",
    "# df_not_arg = pd.read_csv(\"redações/Unlabeled/unlabeled.csv\")\n",
    "# all_df = pd.concat([df, df_not_arg], ignore_index=True)\n",
    "# content = list(all_df.T.to_dict().values())\n",
    "\n",
    "# nlx-group:\n",
    "df = pd.read_csv(\"nlx-data-arguments/data-nlx-group.csv\")\n",
    "content = list(df.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation \\\n",
    "        .replace('-', '') \\\n",
    "        .replace('_', '')\n",
    "punctuation += \"—«»\"\n",
    "punctuation = r\"[{}]\".format(punctuation)\n",
    "re_trim = re.compile(r' +', re.UNICODE)\n",
    "\n",
    "def loadStopWordsPT(filename):\n",
    "    lines = [line.rstrip('\\n').strip() for line in open(filename)]\n",
    "    return lines\n",
    "\n",
    "        \n",
    "def clean_stopwords(text, stop_words_list):\n",
    "    list_words = clean(text).split()\n",
    "    list_clean_text = []\n",
    "    for word in list_words:\n",
    "        if word not in stop_words_list:\n",
    "            list_clean_text.append(word)\n",
    "    return \" \".join(list_clean_text)\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(punctuation, \" \", text)\n",
    "    text = re_trim.sub(' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_stop_words = loadStopWordsPT('data/stop_words_preprocessed.txt')\n",
    "argumentacao = clean_stopwords(argumentacao, raw_stop_words)\n",
    "\n",
    "for i in range(len(content)):\n",
    "    content[i]['text'] = clean_stopwords(content[i]['text'], raw_stop_words)\n",
    "    sent_text = nltk.sent_tokenize(content[i]['text'])\n",
    "    assert len(sent_text) <= 1\n",
    "    content[i]['text'] = [sentence.encode(\"utf-8\") for sentence in sent_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 710,\n",
       " 'label': 'argumento',\n",
       " 'text': [b'nesse caso nunca podemos fazer mal atacar matar diretamente crian\\xc3\\xa7a atrav\\xc3\\xa9s aborto bem salvando vida m\\xc3\\xa3e possa resultar']}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_text(list_sentences):\n",
    "    original_text = ''\n",
    "    for sentence in list_sentences:\n",
    "        original_text += sentence.decode(\"utf-8\")\n",
    "    return original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dirty_sentences(list_sentences):\n",
    "    clean_sentences = []\n",
    "    for sentence in list_sentences:\n",
    "        if(len(sentence.split()) > 2):\n",
    "            clean_sentences.append(sentence)\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(content)):\n",
    "    content[i]['text'] = remove_dirty_sentences(content[i]['text'])\n",
    "content = [lista for lista in content if len(lista['text']) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1h 7min 17s, sys: 51min 15s, total: 1h 58min 32s\nWall time: 20min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lexicons_rates = list()\n",
    "for sentences in content:\n",
    "    arg_avg = 0\n",
    "\n",
    "    for sentence in sentences['text']:\n",
    "        arg_avg += lexicon_rate(argumentacao, sentence)\n",
    "    arg_avg = arg_avg / float(len(sentences['text']))\n",
    "    \n",
    "    rates = list([sentences['id'], restore_text(sentences['text']),sentences['label'],arg_avg])\n",
    "    lexicons_rates.append(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lexicons_rates, columns=['id','text','label','wmd_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv(f\"body/redacoes_corretas/wmd_rc_{folders[n_folder].split('/')[1]}.csv\", index=False)\n",
    "# df.to_csv(f\"body/redacoes_corretas/wmd_rc_all_indicators_wocluewords.csv\", index=False)\n",
    "df.to_csv(f\"body/nlx/wmd_nlx_all_indicators_woclue.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "os.system('spd-say \"cabou\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# wv.wmdistance()\n",
    "wv.wmdistance(['a'], ['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}