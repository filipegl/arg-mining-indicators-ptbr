{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from pyemd import emd\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value(w):\n",
    "    w_string = str(w.encode('utf-8'))\n",
    "    if(w_string in vocab_dict):\n",
    "        return(vocab_dict[w_string])\n",
    "        \n",
    "    return 0\n",
    "\n",
    "def lexicon_rate(lexicon, comment):\n",
    "    vect = CountVectorizer(token_pattern=pattern, strip_accents=None).fit([lexicon, comment])\n",
    "    v_1, v_2 = vect.transform([lexicon, comment])\n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    W_ = W[[check_value(w) for w in vect.get_feature_names()]]\n",
    "    D_ = euclidean_distances(W_)\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max()\n",
    "    lex=emd(v_1, v_2, D_)\n",
    "    return(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 57s, sys: 3.08 s, total: 2min\nWall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv = KeyedVectors.load_word2vec_format('embedding/w2v-v1.wordvectors', unicode_errors=\"ignore\")\n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "fp = np.memmap(\"data/embed.dat\", dtype=np.double, mode='w+', shape=wv.vectors_norm.shape)\n",
    "fp[:] = wv.vectors_norm[:]\n",
    "with open(\"data/embed.vocab\", \"w\") as f:\n",
    "    for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "        print(w.encode('utf-8'), file=f)\n",
    "\n",
    "vocab_len = len(wv.vocab)\n",
    "del fp\n",
    "\n",
    "W = np.memmap(\"data/embed.dat\", dtype=np.double, mode=\"r\", shape=(vocab_len, 300))\n",
    "\n",
    "with open(\"data/embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n",
    "vocab_dict={w:k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para cada fonte, percorrer a lista de arquivos em determianda pasta\n",
    "folders = [\n",
    "    'Indicadores/brasil_escola',\n",
    "    'Indicadores/clue_words_pt',\n",
    "    'Indicadores/acrobata_das_letras',\n",
    "    'Indicadores/mundo_educacao',\n",
    "    'Indicadores/Subjetivity_lexicon',\n",
    "    'Indicadores/PersuativeEssays_UKP',\n",
    "    'Indicadores/2020_Jonathan'\n",
    "]\n",
    "\n",
    "\n",
    "def get_indicators_by_sources(folders):\n",
    "    indicators = dict()\n",
    "    for folder in folders:\n",
    "        argumentacao = list()\n",
    "        for file_name in os.listdir(folder):    \n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            with open(file_path) as f:\n",
    "                argumentacao += [w.rstrip() for w in f.readlines()]\n",
    "        argumentacao = ' '.join(argumentacao)\n",
    "        indicators[folder.split('/')[1]] = argumentacao\n",
    "    return indicators\n",
    "    \n",
    "def get_all_indicators():\n",
    "    with open('Indicadores/preprocessed_indicators.txt') as f:\n",
    "        argumentacao = [w.rstrip() for w in f.readlines()]\n",
    "    return ' '.join(argumentacao)\n",
    "\n",
    "argumentacoes = get_indicators_by_sources(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentacao = argumentacoes[folders[6].split('/')[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"redações/redacoes_extraidas.csv\")\n",
    "df_not_arg = pd.read_csv(\"redações/Unlabeled/unlabeled.csv\")\n",
    "all_df = pd.concat([df, df_not_arg], ignore_index=True)\n",
    "content = list(all_df.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation \\\n",
    "        .replace('-', '') \\\n",
    "        .replace('_', '')\n",
    "punctuation += \"—«»\"\n",
    "punctuation = r\"[{}]\".format(punctuation)\n",
    "re_trim = re.compile(r' +', re.UNICODE)\n",
    "\n",
    "def loadStopWordsPT(filename):\n",
    "    lines = [line.rstrip('\\n').strip() for line in open(filename)]\n",
    "    return lines\n",
    "\n",
    "raw_stop_words = loadStopWordsPT('data/stop_words_preprocessed.txt')\n",
    "        \n",
    "def clean_stopwords(text, stop_words_list):\n",
    "    list_words = text.split()\n",
    "    list_clean_text = []\n",
    "    for word in list_words:\n",
    "        if word not in stop_words_list:\n",
    "            list_clean_text.append(word)\n",
    "    return \" \".join(list_clean_text)\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(punctuation, \" \", text)\n",
    "    text = re_trim.sub(' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentacao = clean_stopwords(argumentacao, raw_stop_words)\n",
    "\n",
    "for i in range(len(content)):\n",
    "    content[i]['text'] = clean_stopwords(content[i]['text'], raw_stop_words)\n",
    "    content[i]['text'] = clean(content[i]['text'])\n",
    "    # content[i] = unicode(content[i], \"utf-8\")\n",
    "    sent_text = nltk.sent_tokenize(content[i]['text'])\n",
    "    content[i]['text'] = [sentence.encode(\"utf-8\") for sentence in sent_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 697,\n",
       " 'label': 'nao_argumento',\n",
       " 'text': [b'aspectos devem considerados rela\\xc3\\xa7\\xc3\\xa3o isto legado hist\\xc3\\xb3rico-cultural desrespeito leis']}"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "content[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_text(list_sentences):\n",
    "    original_text = ''\n",
    "    for sentence in list_sentences:\n",
    "        original_text += sentence.decode(\"utf-8\")\n",
    "    return original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dirty_sentences(list_sentences):\n",
    "    clean_sentences = []\n",
    "    for sentence in list_sentences:\n",
    "        if(len(sentence.split()) > 2):\n",
    "            clean_sentences.append(sentence)\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(content)):\n",
    "    content[i]['text'] = remove_dirty_sentences(content[i]['text'])\n",
    "content = [lista for lista in content if len(lista['text']) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lexicons_rates = list()\n",
    "for sentences in content:\n",
    "    arg_avg = 0\n",
    "\n",
    "    for sentence in sentences['text']:\n",
    "        arg_avg += lexicon_rate(argumentacao, sentence)\n",
    "    arg_avg = arg_avg / float(len(sentences['text']))\n",
    "    \n",
    "    rates = list([sentences['id'], restore_text(sentences['text']),sentences['label'],arg_avg])\n",
    "    lexicons_rates.append(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lexicons_rates, columns=['id','text','label','wmd_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"body/wmd_{folders[6].split('/')[1]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# wv.wmdistance()\n",
    "wv.wmdistance(['a'], ['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}